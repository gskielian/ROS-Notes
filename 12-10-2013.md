Tools

* build system (catkin)
* parameter server (rosparam)
* recording and playback (rosbag)
* visualization (rviz)
* monitoring (rqt)


## rosbag




## rviz

has markers, in visualization window can see what the robot is seeing, and can click on the arm and drag it and move it (if the PR2).
Can interact with the robot this way.

can make a 3d joystick and can drag with the joystick and interact how you want...

difficult to implement on one's own

> There was an app to run a turtle bot camera with navigation 

Completely open-source


Quotes from Meeting
> One of the problems is that ROS was built to run on quad-core monster cpu's
> No Problem running basic navigation stack on the Raspberry Pi
> 


## RQT -- Monitoring


Integration with other libraries
* Gazebo (simulator)
  * physics simulations 
  * laser range finders
* OpenCV (computer vision)
  * really well integrated into ROS
  * face-detection
  * people-detection
  * range-detection
  * edge-detection
* PCL (point cloud library)
  * Really useful when using kinect sensors 
  * or point cloud sensors (like laser + dual-axis diffraction grating)
* MoveIt (Motion planning library)

> Especially for doing pick-and-place arm motions
> Center of gravity, turn of radius, how that will change the direction of the robot

## Further REadings


http://wiki.ros.org/ROS/Tutorials
